{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#todo: change private function name\n",
    "#todo: change calling function to arg = arg\n",
    "\n",
    "class Retrieve_internal:\n",
    "    def __init__(self, openAi_key: str) -> None:\n",
    "        with open(\"data/scrapData.pkl\",\"rb\") as file:\n",
    "            self.internal_docs = pickle.load(file)\n",
    "        with open(\"data/128Embedding.npy\") as file:\n",
    "            self.internal_docs_embeddings = np.load(file)\n",
    "        self.embeddings_model = OpenAIEmbeddings(openai_api_key=openAi_key)\n",
    "\n",
    "    \n",
    "    def retrieve_internal(self, input: str, max_length: int) -> str:\n",
    "        input_embedding = self.embeddings_model.embed_documents([input])\n",
    "        dot_distances = np.dot(self.internal_docs_embeddings, np.array(input_embedding).transpose())\n",
    "        dot_distances = [(dot_distances[i],i) for i in range(len(dot_distances))]   \n",
    "        dot_distances.sort(reverse=True)\n",
    "        ret = ''\n",
    "        append_idx = 0\n",
    "        while len(ret) < max_length:\n",
    "            ret += self.internal_docs[dot_distances[append_idx][1]] + '\\n'\n",
    "            append_idx += 1\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "\n",
    "class Partselect_scrapper:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def get_video_title(self, video_ID: str) -> str:\n",
    "        url = 'https://noembed.com/embed?url=https://www.youtube.com/watch?v='+video_ID\n",
    "        result = requests.get(url)\n",
    "\n",
    "        toJson = json.loads(result.content)\n",
    "        video_title = toJson.get('title')\n",
    "        return video_title\n",
    "\n",
    "    def get_youtube_title_and_trasncript(self, video_ID: str) -> tuple():\n",
    "        transcript_result = YouTubeTranscriptApi.get_transcript(video_ID)\n",
    "        transcript = ''\n",
    "        for d in transcript_result:\n",
    "            transcript += d['text'] + ' '\n",
    "        transcript = transcript.replace('\\n', ' ')\n",
    "        title = self.get_video_title(video_ID)\n",
    "        transcript = \"vido title: \"+ title + '\\n' + \"Transcript:\\n \" + transcript\n",
    "        return (title,transcript)\n",
    "\n",
    "    def get_page_video_ID(self, soup: BeautifulSoup) -> list:\n",
    "    \n",
    "        labels = ['div']\n",
    "        classes = ['yt-video']\n",
    "        results = soup.find_all(labels,class_=classes)\n",
    "        viedo_ID_set = set()\n",
    "        for result in results:\n",
    "            # Extract and print details from each result\n",
    "            video_ID = result['data-yt-init']\n",
    "            if video_ID == \"d6AvOkulk_g\":\n",
    "                continue\n",
    "            viedo_ID_set.add(video_ID)\n",
    "        return list(viedo_ID_set)\n",
    "\n",
    "    def get_all_title_and_transcript(self, video_ID_list: list) -> list:\n",
    "        ret = []\n",
    "        for id in video_ID_list:\n",
    "            try:\n",
    "                ret.append(self.get_youtube_title_and_trasncript(id))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        return ret\n",
    "    @classmethod\n",
    "    def search_part(self, query: str, get_video = True) -> str:\n",
    "        # Inspect the website to find the correct URL and parameters\n",
    "        url = 'https://www.partselect.com/api/search/'\n",
    "        params = {'searchterm': query}\n",
    "        response = requests.get(url, params=params)\n",
    "        ret = ''\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                # Parse the soup object to find search results\n",
    "                # This depends on the HTML structure of the search results page\n",
    "                #results = soup.find_all('div', class_='search-result')  # Example, adjust based on actual page structure\n",
    "                labels = ['div','h1']\n",
    "                classes = ['pd__description','title-lg','title-main','repair-story__instruction','col-md-6 mt-3']\n",
    "                results = soup.find_all(labels,class_=classes)\n",
    "                printed_story = False\n",
    "                printed_trouble_shooting = False\n",
    "                for result in results:\n",
    "                    # Extract and print details from each result\n",
    "                    if 'title-lg' in result['class'] or 'title-main' in result['class']:\n",
    "                        ret += \"Name: \"+ result.get_text(strip=True)\n",
    "                    elif 'pd__description' in result['class']:\n",
    "                        description_title = result.find('h2', class_='title-md').get_text(strip=True)\n",
    "                        description = result.find('div', itemprop='description').get_text(strip=True)\n",
    "                        ret += description_title\n",
    "                        ret += description\n",
    "\n",
    "                    elif 'repair-story__instruction' in result['class']:\n",
    "                        if not printed_story:\n",
    "                            ret += \"Repair Story From Customer:\"\n",
    "                            printed_story = True\n",
    "                        ret += result.get_text(strip=True)\n",
    "                    else:\n",
    "                        if not printed_trouble_shooting:\n",
    "                            ret += \"Trouble Shooting:\"\n",
    "                            printed_trouble_shooting = True\n",
    "                        ret += result.get_text(strip=True)\n",
    "                if get_video:\n",
    "                    video_id_list = self.get_page_video_ID(soup)\n",
    "                    title_and_transcript_list = self.get_all_title_and_transcript(video_ID_list=video_id_list)\n",
    "                    for _, transcript in title_and_transcript_list:\n",
    "                        ret += '\\n' + transcript\n",
    "                return ret\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                return \"\"\n",
    "        else:\n",
    "            print(\"Partselect_scrapper: network error\")\n",
    "            return \"\"\n",
    "        \n",
    "    def get_compatible_parts(self, source_part_ID: str, query: str):\n",
    "        params = {\"SearchTerm\": query}\n",
    "        url = \"https://www.partselect.com/Models/\" + source_part_ID + \"/Parts/\"\n",
    "        response = requests.get(url, params)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        parts_divs = soup.find_all('div', class_='mega-m__part')\n",
    "\n",
    "        parts_list = []\n",
    "        for part in parts_divs:\n",
    "            part_name = part.find('a', class_='mega-m__part__name')\n",
    "            part_number = part_name.find_next_sibling('div')\n",
    "            if part_number:\n",
    "                parts_list.append(part_number.get_text(strip=True).split(':')[-1])\n",
    "        if len(part_number) == 0:\n",
    "            return 'could find compatible parts'\n",
    "        ret = 'Compatible parts found: \\n'\n",
    "        for part_number in parts_list:\n",
    "            ret += self.search_part(query=part_number, get_video=False)\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from math import sqrt, cos, sin\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "\n",
    "\n",
    "class SearchPartTool(BaseTool):\n",
    "    name = \"Find relevant part for machine\"\n",
    "    description = desc\n",
    "\n",
    "    def _run(self, source_part_ID: str, query: str) -> str:\n",
    "        params = {\"SearchTerm\": query}\n",
    "        url = \"https://www.partselect.com/Models/\" + source_part_ID + \"/Parts/\"\n",
    "        response = requests.get(url, params)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        parts_divs = soup.find_all('div', class_='mega-m__part')\n",
    "\n",
    "        parts_list = []\n",
    "        for part in parts_divs:\n",
    "            part_name = part.find('a', class_='mega-m__part__name')\n",
    "            part_number = part_name.find_next_sibling('div')\n",
    "            if part_number:\n",
    "                parts_list.append(part_number.get_text(strip=True).split(':')[-1])\n",
    "        if len(part_number) == 0:\n",
    "            return 'could find compatible parts'\n",
    "        ret = 'Compatible parts found: \\n'\n",
    "        for part_number in parts_list:\n",
    "            ret += Retrieve_internal.search_part(query=part_number, get_video=False)\n",
    "        return ret\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "\n",
    "\n",
    "class RelevantPartTool(BaseTool):\n",
    "    name = \"Find relevant part for machine\"\n",
    "    description = (\n",
    "    \"Use this tool when you need to find related part of a machine. \"\n",
    "    \"To use the tool you must provide exactly two argument ['model number, query']. \"\n",
    "    \"The first argument must be a model number of a machine, such as WDT780SAEM1. \" \n",
    "    \"The second argument can be a part number such as PS3406971, or a description, such as dishrack wheel. \"\n",
    "    \"For exmaple: \" \n",
    "    \"question: I want a drawer track for FPHD2491KF0. Arugment = [FPHD2491KF0, drawer track], \"\n",
    "    \"question: is PS429725 compatible with my FGHS2631PF4A. Arumgnet = [FGHS2631PF4A, PS429725]\")\n",
    "\n",
    "    def _run(self, source_part_ID: str, query: str) -> str:\n",
    "        params = {\"SearchTerm\": query}\n",
    "        url = \"https://www.partselect.com/Models/\" + source_part_ID + \"/Parts/\"\n",
    "        response = requests.get(url, params)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        parts_divs = soup.find_all('div', class_='mega-m__part')\n",
    "\n",
    "        parts_list = []\n",
    "        for part in parts_divs:\n",
    "            part_name = part.find('a', class_='mega-m__part__name')\n",
    "            part_number = part_name.find_next_sibling('div')\n",
    "            if part_number:\n",
    "                parts_list.append(part_number.get_text(strip=True).split(':')[-1])\n",
    "        if len(part_number) == 0:\n",
    "            return 'could find compatible parts'\n",
    "        ret = 'Compatible parts found: \\n'\n",
    "        for part_number in parts_list:\n",
    "            ret += Retrieve_internal.search_part(query=part_number, get_video=False)\n",
    "        return ret\n",
    "    \n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "\n",
    "tools = [RelevantPartTool()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
